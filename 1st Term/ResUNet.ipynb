{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import scipy.signal\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.utils.data.distributed\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # Configuring Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RamanDataset(Dataset):\n",
    "    def __init__(self, inputs, outputs, batch_size=64,spectrum_len=500, spectrum_shift=0., \n",
    "                 spectrum_window=False, horizontal_flip=False, mixup=False):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.batch_size = batch_size\n",
    "        self.spectrum_len = spectrum_len\n",
    "        self.spectrum_shift = spectrum_shift\n",
    "        self.spectrum_window = spectrum_window\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.mixup = mixup\n",
    "        self.on_epoch_end()     \n",
    "        \n",
    "    def pad_spectrum(self, input_spectrum, spectrum_length):\n",
    "        if len(input_spectrum) == spectrum_length:\n",
    "            padded_spectrum = input_spectrum\n",
    "        elif len(input_spectrum) > spectrum_length:\n",
    "            padded_spectrum = input_spectrum[0:spectrum_length]\n",
    "        else:\n",
    "            padded_spectrum = np.pad(input_spectrum, ((0,spectrum_length - len(input_spectrum)),(0,0)), 'reflect')\n",
    "\n",
    "        return padded_spectrum\n",
    "    \n",
    "    def window_spectrum(self, input_spectrum, start_idx, window_length):\n",
    "        if len(input_spectrum) <= window_length:\n",
    "            output_spectrum = input_spectrum\n",
    "        else:\n",
    "            end_idx = start_idx + window_length\n",
    "            output_spectrum = input_spectrum[start_idx:end_idx]\n",
    "\n",
    "        return output_spectrum\n",
    "            \n",
    "    def flip_axis(self, x, axis):\n",
    "        if np.random.random() < 0.5:\n",
    "            x = np.asarray(x).swapaxes(axis, 0)\n",
    "            x = x[::-1, ...]\n",
    "            x = x.swapaxes(0, axis)\n",
    "        return x\n",
    "    \n",
    "    def shift_spectrum(self, x, shift_range):\n",
    "        x = np.expand_dims(x,axis=-1)\n",
    "        shifted_spectrum = x\n",
    "        spectrum_shift_range = int(np.round(shift_range*len(x)))\n",
    "        if spectrum_shift_range > 0:\n",
    "            shifted_spectrum = np.pad(x[spectrum_shift_range:,:], ((0,abs(spectrum_shift_range)), (0,0)), 'reflect')\n",
    "        elif spectrum_shift_range < 0:\n",
    "            shifted_spectrum = np.pad(x[:spectrum_shift_range,:], ((abs(spectrum_shift_range), 0), (0,0)), 'reflect')\n",
    "        return shifted_spectrum\n",
    "    \n",
    "    def mixup_spectrum(self, input_spectrum1, input_spectrum2, output_spectrum1, output_spectrum2, alpha):\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        input_spectrum = (lam * input_spectrum1) + ((1 - lam) * input_spectrum2)\n",
    "        output_spectrum = (lam * output_spectrum1) + ((1 - lam) * output_spectrum2)\n",
    "        return input_spectrum, output_spectrum\n",
    "            \n",
    "    def __getitem__(self, index):       \n",
    "        input_spectrum = self.inputs[index]\n",
    "        output_spectrum = self.outputs[index]\n",
    "        \n",
    "        mixup_on = False\n",
    "        if self.mixup:\n",
    "            if np.random.random() < 0.5:\n",
    "                spectrum_idx = int(np.round(np.random.random() * (len(self.inputs)-1)))\n",
    "                input_spectrum2 = self.inputs[spectrum_idx]\n",
    "                output_spectrum2 = self.outputs[spectrum_idx]\n",
    "                mixup_on = True\n",
    "\n",
    "        if self.spectrum_window:\n",
    "            start_idx = int(np.floor(np.random.random() * (len(input_spectrum)-self.spectrum_len)))\n",
    "            input_spectrum = self.window_spectrum(input_spectrum, start_idx, self.spectrum_len)\n",
    "            output_spectrum = self.window_spectrum(output_spectrum, start_idx, self.spectrum_len)\n",
    "            if mixup_on:\n",
    "                input_spectrum2 = self.window_spectrum(input_spectrum2, start_idx, self.spectrum_len)\n",
    "                output_spectrum2 = self.window_spectrum(output_spectrum2, start_idx, self.spectrum_len)\n",
    "\n",
    "        input_spectrum = self.pad_spectrum(input_spectrum, self.spectrum_len)\n",
    "        output_spectrum = self.pad_spectrum(output_spectrum, self.spectrum_len)\n",
    "        if mixup_on:\n",
    "            input_spectrum2 = self.pad_spectrum(input_spectrum2, self.spectrum_len)\n",
    "            output_spectrum2 = self.pad_spectrum(output_spectrum2, self.spectrum_len)\n",
    "\n",
    "        if self.spectrum_shift != 0.0:\n",
    "            shift_range = np.random.uniform(-self.spectrum_shift, self.spectrum_shift)\n",
    "            input_spectrum = self.shift_spectrum(input_spectrum, shift_range)\n",
    "            output_spectrum = self.shift_spectrum(output_spectrum, shift_range)\n",
    "            if mixup_on:\n",
    "                input_spectrum2 = self.shift_spectrum(input_spectrum2, shift_range)\n",
    "                output_spectrum2 = self.shift_spectrum(output_spectrum2, shift_range)\n",
    "        else:\n",
    "            input_spectrum = np.expand_dims(input_spectrum, axis=-1)\n",
    "            output_spectrum = np.expand_dims(output_spectrum, axis=-1)\n",
    "            if mixup_on:\n",
    "                input_spectrum2 = np.expand_dims(input_spectrum2, axis=-1)\n",
    "                output_spectrum2 = np.expand_dims(output_spectrum2, axis=-1)\n",
    "\n",
    "        if self.horizontal_flip:    \n",
    "            if np.random.random() < 0.5:\n",
    "                input_spectrum = self.flip_axis(input_spectrum, 0)\n",
    "                output_spectrum = self.flip_axis(output_spectrum, 0)\n",
    "                if mixup_on:\n",
    "                    input_spectrum2 = self.flip_axis(input_spectrum2, 0)\n",
    "                    output_spectrum2 = self.flip_axis(output_spectrum2, 0)\n",
    "\n",
    "        if mixup_on:\n",
    "            input_spectrum, output_spectrum = self.mixup_spectrum(input_spectrum, input_spectrum2, output_spectrum, output_spectrum2, 0.2)\n",
    "            \n",
    "        input_spectrum = input_spectrum/np.amax(input_spectrum)\n",
    "        output_spectrum = output_spectrum/np.amax(output_spectrum)\n",
    "        \n",
    "        input_spectrum = np.moveaxis(input_spectrum, -1, 0)\n",
    "        output_spectrum = np.moveaxis(output_spectrum, -1, 0)\n",
    "        \n",
    "        sample = {'input_spectrum': input_spectrum, 'output_spectrum': output_spectrum}\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out, batch_norm):\n",
    "        super(BasicConv, self).__init__()\n",
    "        basic_conv = [nn.Conv1d(channels_in, channels_out, kernel_size = 3, stride = 1, padding = 1, bias = True)]\n",
    "        basic_conv.append(nn.PReLU())\n",
    "        if batch_norm:\n",
    "            basic_conv.append(nn.BatchNorm1d(channels_out))\n",
    "        \n",
    "        self.body = nn.Sequential(*basic_conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.body(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResUNetConv(nn.Module):\n",
    "    def __init__(self, num_convs, channels, batch_norm):\n",
    "        super(ResUNetConv, self).__init__()\n",
    "        unet_conv = []\n",
    "        for _ in range(num_convs):\n",
    "            unet_conv.append(nn.Conv1d(channels, channels, kernel_size = 3, stride = 1, padding = 1, bias = True))\n",
    "            unet_conv.append(nn.PReLU())\n",
    "            if batch_norm:\n",
    "                unet_conv.append(nn.BatchNorm1d(channels))\n",
    "        \n",
    "        self.body = nn.Sequential(*unet_conv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetLinear(nn.Module):\n",
    "    def __init__(self, repeats, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        for i in range(repeats):\n",
    "            modules.append(nn.Linear(channels_in, channels_out))\n",
    "            modules.append(nn.PReLU())\n",
    "        \n",
    "        self.body = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResUNet(nn.Module):\n",
    "    def __init__(self, num_convs, batch_norm):\n",
    "        super(ResUNet, self).__init__()\n",
    "        res_conv1 = [BasicConv(1, 64, batch_norm)]\n",
    "        res_conv1.append(ResUNetConv(num_convs, 64, batch_norm))\n",
    "        self.conv1 = nn.Sequential(*res_conv1)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "\n",
    "        res_conv2 = [BasicConv(64, 128, batch_norm)]\n",
    "        res_conv2.append(ResUNetConv(num_convs, 128, batch_norm))\n",
    "        self.conv2 = nn.Sequential(*res_conv2)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "\n",
    "        res_conv3 = [BasicConv(128, 256, batch_norm)]\n",
    "        res_conv3.append(ResUNetConv(num_convs, 256, batch_norm))\n",
    "        res_conv3.append(BasicConv(256, 128, batch_norm))\n",
    "        self.conv3 = nn.Sequential(*res_conv3)       \n",
    "        self.up3 = nn.Upsample(scale_factor = 2)\n",
    "\n",
    "        res_conv4 = [BasicConv(256, 128, batch_norm)]\n",
    "        res_conv4.append(ResUNetConv(num_convs, 128, batch_norm))\n",
    "        res_conv4.append(BasicConv(128, 64, batch_norm))\n",
    "        self.conv4 = nn.Sequential(*res_conv4)     \n",
    "        self.up4 = nn.Upsample(scale_factor = 2)\n",
    "\n",
    "        res_conv5 = [BasicConv(128, 64, batch_norm)]\n",
    "        res_conv5.append(ResUNetConv(num_convs,64, batch_norm))\n",
    "        self.conv5 = nn.Sequential(*res_conv5)\n",
    "        res_conv6 = [BasicConv(64, 1, batch_norm)]\n",
    "        self.conv6 = nn.Sequential(*res_conv6)    \n",
    "        \n",
    "        self.linear7 = UNetLinear(3, 500, 500)\n",
    "               \n",
    "    def forward(self, x):\n",
    "        x.to(device)\n",
    "        x = self.conv1(x)\n",
    "        x1 = self.pool1(x)\n",
    "        \n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.pool1(x2)\n",
    "        \n",
    "        x3 = self.conv3(x3)\n",
    "        x3 = self.up3(x3)\n",
    "        \n",
    "        x4 = torch.cat((x2, x3), dim = 1)\n",
    "        x4 = self.conv4(x4)\n",
    "        x5 = self.up4(x4)\n",
    "\n",
    "        x6 = torch.cat((x, x5), dim = 1)\n",
    "        x6 = self.conv5(x6)\n",
    "        x7 = self.conv6(x6)\n",
    "        \n",
    "        out = self.linear7(x7)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressMeter(object):\n",
    "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
    "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
    "        self.meters = meters\n",
    "        self.prefix = prefix\n",
    "\n",
    "    def display(self, batch):\n",
    "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
    "        entries += [str(meter) for meter in self.meters]\n",
    "        print('\\t'.join(entries))\n",
    "\n",
    "    def _get_batch_fmtstr(self, num_batches):\n",
    "        num_digits = len(str(num_batches // 1))\n",
    "        fmt = '{:' + str(num_digits) + 'd}'\n",
    "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    args.gpu = gpu\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "\n",
    "    if args.distributed:\n",
    "        if args.dist_url == \"env://\" and args.rank == -1:\n",
    "            args.rank = int(os.environ[\"RANK\"])\n",
    "        if args.multiprocessing_distributed:\n",
    "            args.rank = args.rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                world_size=args.world_size, rank=args.rank)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # Create model(s) and send to device(s)\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    net = ResUNet(3, False).float()\n",
    "    net.load_state_dict(torch.load('ResUNet.pt'))\n",
    "\n",
    "    if args.distributed:\n",
    "        if args.gpu is not None:\n",
    "            torch.cuda.set_device(args.gpu)\n",
    "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
    "            args.workers = int((args.workers + ngpus_per_node - 1) / ngpus_per_node)\n",
    "\n",
    "            net.cuda(args.gpu)\n",
    "            net = torch.nn.parallel.DistributedDataParallel(net, device_ids=[args.gpu])\n",
    "        else:\n",
    "            net.cuda(args.gpu)\n",
    "            net = torch.nn.parallel.DistributedDataParallel(net)\n",
    "    elif args.gpu is not None:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        net.cuda(args.gpu)\n",
    "    else:\n",
    "        net.cuda(args.gpu)\n",
    "        net = torch.nn.parallel.DistributedDataParallel(net)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # Define dataset path and data splits\n",
    "    # ----------------------------------------------------------------------------------------    \n",
    "    Input_Data = scipy.io.loadmat(\"\\Path\\To\\Inputs.mat\")\n",
    "    Output_Data = scipy.io.loadmat(\"\\Path\\To\\Outputs.mat\")\n",
    "\n",
    "    Input = Input_Data['data']\n",
    "    Output = Output_Data['data']\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # Create datasets (with augmentation) and dataloaders\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    Raman_Dataset_Test = RamanDataset(Input, Output, batch_size = args.batch_size, spectrum_len = args.spectrum_len)\n",
    "\n",
    "    test_loader = DataLoader(Raman_Dataset_Test, batch_size = args.batch_size, shuffle = False, num_workers = 0, pin_memory = True)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    # Evaluate\n",
    "    # ----------------------------------------------------------------------------------------\n",
    "    MSE_NN, MSE_SG = evaluate(test_loader, net, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(dataloader, net):\n",
    "    losses = AverageMeter('Loss', ':.4e')\n",
    "    SG_loss = AverageMeter('Savitzky-Golay Loss', ':.4e')\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    MSE_SG = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader):\n",
    "            x = data['input_spectrum']\n",
    "            inputs = x.float()\n",
    "            print(x)\n",
    "            y = data['output_spectrum']\n",
    "            target = y.float()\n",
    "            \n",
    "            x = np.squeeze(x.numpy())\n",
    "            y = np.squeeze(y.numpy())\n",
    "\n",
    "            output = net(inputs)\n",
    "            loss = nn.MSELoss()(output, target)\n",
    "            \n",
    "            x_out = output.cpu().detach().numpy()\n",
    "            x_out = np.squeeze(x_out)\n",
    "\n",
    "            SGF_1_9 = scipy.signal.savgol_filter(x,9,1)\n",
    "            MSE_SGF_1_9 = np.mean(np.mean(np.square(np.absolute(y - (SGF_1_9 - np.reshape(np.amin(SGF_1_9, axis = 1), (len(SGF_1_9),1)))))))\n",
    "            MSE_SG.append(MSE_SGF_1_9)\n",
    "            \n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "\n",
    "        print(\"Neural Network MSE: {}\".format(losses.avg))\n",
    "        print(\"Savitzky-Golay MSE: {}\".format(np.mean(np.asarray(MSE_SG))))\n",
    "        print(\"Neural Network performed {0:.2f}x better than Savitzky-Golay\".format(np.mean(np.asarray(MSE_SG))/losses.avg))\n",
    "\n",
    "    return losses.avg, MSE_SG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = ResUNet(3, False).float().to(device)\n",
    "net.load_state_dict(torch.load('ResUNet.pt', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3_1 = scio.loadmat('tissue3_1.mat')\n",
    "data = data3_1['map_t3']\n",
    "data = data.reshape(40000,1024)\n",
    "out_data = data3_1['bcc']\n",
    "out_data = out_data.reshape(40000,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Raman_Dataset_Test = RamanDataset(data, data, spectrum_len = 1024)\n",
    "test_loader = DataLoader(Raman_Dataset_Test, batch_size = None, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9611, 0.9592, 0.9614,  ..., 0.9932, 0.9944, 1.0000]],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight [64, 1, 3], but got 2-dimensional input of size [1, 1024] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tl/_smmzcf56jn3pnwyc8ggqhdc0000gn/T/ipykernel_10224/2551963110.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMSE_NN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE_SG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/tl/_smmzcf56jn3pnwyc8ggqhdc0000gn/T/ipykernel_10224/3547848452.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataloader, net)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tl/_smmzcf56jn3pnwyc8ggqhdc0000gn/T/ipykernel_10224/3292784766.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/tl/_smmzcf56jn3pnwyc8ggqhdc0000gn/T/ipykernel_10224/367965069.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    295\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[0;32m--> 297\u001b[0;31m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    298\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight [64, 1, 3], but got 2-dimensional input of size [1, 1024] instead"
     ]
    }
   ],
   "source": [
    "MSE_NN, MSE_SG = evaluate(test_loader, net)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
